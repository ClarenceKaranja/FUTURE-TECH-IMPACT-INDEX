{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sz87uYr1znV60EBXODYY9x9xpFeFYT0u",
      "authorship_tag": "ABX9TyMG5FcjPvl4yQn5Fj6mhNqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClarenceKaranja/FUTURE-TECH-IMPACT-INDEX/blob/main/RECOMMENDER_SYSTEM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Title:**\n",
        "## **NAVIGATING TOMORROW: INTRODUCING THE FUTURE TECH IMPACT INDEX**\n",
        "\n",
        "In the swift current of technological advancement, staying ahead requires more than just keeping up—it demands foresight. Welcome to the \"Future Tech Impact Index,\" a revolutionary project set to transform decision-making in the ever-changing landscape of emerging technologies.\n",
        "\n",
        "**A Glimpse into the Future:**\n",
        "The \"Future Tech Impact Index\" isn't just a tool; it's a compass designed to guide businesses, investors, and policymakers through the complexities of tomorrow's innovations.\n",
        "\n",
        "**Data-Driven Insights:**\n",
        "This index leverages data science to provide a comprehensive assessment of emerging technologies. It's not just about information; it's about actionable insights.\n",
        "\n",
        "**Empowering Decision-Making:**\n",
        "Imagine having a tool that not only empowers businesses with strategic insights but also lends expertise to policymakers crafting regulations for sustainable progress. That's the \"Future Tech Impact Index.\"\n",
        "\n",
        "**How It Works in a Snapshot:**\n",
        "Automated data collection, meticulous methodology, and a keen alignment with societal goals—this index simplifies the complex journey from information to impact scores.\n",
        "\n",
        "**Illuminate Tomorrow's Transformations:**\n",
        "The recommendations made aren't just ranks; they're a guiding light, revealing the transformative potential of emerging technologies.\n",
        "\n",
        "**In a Nutshell:**\n",
        "The \"Future Tech Impact Index\" is your key to navigating the future with confidence. In a world where innovation is constant, this index is your strategic ally—a concise, powerful guide to informed decisions and sustainable progress. Welcome to a new era of technological foresight—welcome to the future."
      ],
      "metadata": {
        "id": "TFAMDW3OVaom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Downloading NLTK Resources\n",
        "This section ensures that the necessary NLTK resource for tokenization is available.\n",
        "\n"
      ],
      "metadata": {
        "id": "QRFOTOfWKyVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print('Downloading punkt...')\n",
        "    nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLxEL4PI_vJE",
        "outputId": "e956dd30-b826-49f0-deda-e34404ae9e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading punkt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Importing Libraries\n",
        "This section imports the necessary libraries for data processing and analysis."
      ],
      "metadata": {
        "id": "KVI1nyJ9LveO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import numpy\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import re\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "except ImportError:\n",
        "    print('You are missing some packages! ' \\\n",
        "          'We will try installing them before continuing!')\n",
        "    !pip install \"numpy\" \"pandas\" \"sklearn\" \"nltk\"\n",
        "    import numpy\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import re\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGejibywK3LQ",
        "outputId": "188eb2bc-8ce3-4dcc-ed10-612798590c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Set up the Stemmer\n",
        "Initialize the SnowballStemmer from NLTK to stem words in the abstracts."
      ],
      "metadata": {
        "id": "3UWZeWN0Mb_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the stemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "3zx-kDfIFaVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Load and Clean Data\n",
        "Load patent abstracts from a CSV file, select the 'ABSTRACTS' column, and drop rows with missing values."
      ],
      "metadata": {
        "id": "m4NLjpW9Mrjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the patents data\n",
        "PATH_PATENTS = \"/content/drive/MyDrive/content_recommender_data/ABSTRACTS.csv\"\n",
        "patents = pd.read_csv(PATH_PATENTS)\n",
        "patents = patents[['ABSTRACTS']].dropna()  # select the 'abstract' column and drop rows with missing values\n",
        "abstracts = patents['ABSTRACTS'].tolist()  # convert the abstracts to a list"
      ],
      "metadata": {
        "id": "G9sGP36mMqbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Clean and Tokenize Abstracts\n",
        "Define a function to clean and tokenize the abstracts using NLTK and regular expressions."
      ],
      "metadata": {
        "id": "MQYA-YfWM9lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and tokenize the abstracts\n",
        "def clean_tokenize(document):\n",
        "    document = re.sub('[^\\w_\\s-]', ' ', document)  # remove punctuation marks and other symbols\n",
        "    tokens = nltk.word_tokenize(document)  # tokenize sentences\n",
        "    cleaned_abstract = ' '.join([stemmer.stem(item) for item in tokens])  # stem each token\n",
        "    return cleaned_abstract\n",
        "\n",
        "# Apply the cleaning and tokenization function to each abstract.\n",
        "cleaned_abstracts = list(map(clean_tokenize, abstracts))"
      ],
      "metadata": {
        "id": "FXVbFaY-NAMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Get User Input for Keywords\n",
        "Prompt the user to input keywords separated by commas."
      ],
      "metadata": {
        "id": "_H5MTOrWQMl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input for keywords separated by commas\n",
        "user_keywords = input(\"Enter keywords separated by commas and then press Enter: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P2AI0b9FeAJ",
        "outputId": "2ae9bd3c-0c1a-4a79-b317-44a9ef4b49a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter keywords separated by commas and then press Enter: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Section 7: Split User Input into a List of Keywords\n",
        "Split the user input into a list of keywords, removing any leading or trailing whitespaces."
      ],
      "metadata": {
        "id": "OhAAjUCoQgnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split user input into a list of keywords\n",
        "user_keywords_list = [keyword.strip() for keyword in user_keywords.split(',')]\n"
      ],
      "metadata": {
        "id": "F3WmP_xPFgmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: Combine Keywords for Processing\n",
        "Combine the cleaned list of keywords into a single string for further processing."
      ],
      "metadata": {
        "id": "EL4NnaAjRAQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine keywords into a single string for processing\n",
        "cleaned_user_keywords = ' '.join(user_keywords_list)\n"
      ],
      "metadata": {
        "id": "bLk7h0XGQ7RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 9: Process and Clean User Input Keywords\n",
        "Apply the same cleaning and tokenization function used for abstracts to the user input keywords."
      ],
      "metadata": {
        "id": "-WI-k8g0QzuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Process and clean user input keywords\n",
        "cleaned_user_keywords = clean_tokenize(cleaned_user_keywords)\n"
      ],
      "metadata": {
        "id": "CAbg1hulQs68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "abHQhxhmRGDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 10: Generate TF-IDF Matrix for User Input Keywords and All Patents\n",
        "Use the TfidfVectorizer to generate TF-IDF matrix for both user input keywords and all patent abstracts."
      ],
      "metadata": {
        "id": "UiwDBCOcRLMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate TF-IDF matrix for user input keywords and all patents\n",
        "tfidf_matrix = TfidfVectorizer(stop_words='english', min_df=2)\n",
        "abstract_tfidf_matrix = tfidf_matrix.fit_transform(cleaned_abstracts)\n",
        "user_keywords_tfidf_vector = tfidf_matrix.transform([cleaned_user_keywords])\n"
      ],
      "metadata": {
        "id": "kjuk5QkkFjhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 11: Calculate Cosine Similarity\n",
        "Calculate cosine similarity between user input keywords and all patents based on their TF-IDF representations."
      ],
      "metadata": {
        "id": "KJ5SYUX3RQRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between user input keywords and all patents\n",
        "patents_similarity_score = cosine_similarity(abstract_tfidf_matrix, user_keywords_tfidf_vector)\n",
        "recommended_patents_id = patents_similarity_score.flatten().argsort()[::-1]\n"
      ],
      "metadata": {
        "id": "k2if66-9FtSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 12: Define Number of Recommendations to Display\n",
        "Prompt the user to input the number of recommendations they want to display."
      ],
      "metadata": {
        "id": "43Ig8VFbRZGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of recommendations you want to display\n",
        "num_recommendations = int(input(\"Enter the number of recommendations you want to display and then press Enter: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "papW-lHlFqxR",
        "outputId": "3389575e-3ad3-4689-dea9-89620bb28006"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of recommendations you want to display and then press Enter: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 13: Display Top N Recommended Patents\n",
        "Display the top N recommended patents along with their full abstract content."
      ],
      "metadata": {
        "id": "RYqZrngwRgy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recommended_patents = patents.loc[patents.index.isin(recommended_patents_id[:num_recommendations]), 'ABSTRACTS']\n",
        "\n",
        "print(f'\\nTop {num_recommendations} Recommended Patents:')\n",
        "for idx, abstract in zip(recommended_patents.index, recommended_patents):\n",
        "    print(f'\\nPatent ID: {idx}\\nAbstract: {abstract}\\n{\"-\"*50}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMLArSJyFo2x",
        "outputId": "2bd8f3c7-8382-4ce8-c397-e7eeb5bef5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 1 Recommended Patents:\n",
            "\n",
            "Patent ID: 93661\n",
            "Abstract: A deck leverage anchor (40) for securing an external device has an anchor body that is positioned at least partially within an opening of the deck (30) so that a notch receives the edge of the surface. The anchor body comprises a swivel coupler (64’’’) that extends outward from the opening. The swivel coupler couples to an external device. A swivel plate (102) is disposed opposite the swivel coupler to engage and distribute a load on the surface of the deck opposite the coupler.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UihzQMyQT-dh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
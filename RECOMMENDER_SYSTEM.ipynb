{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sz87uYr1znV60EBXODYY9x9xpFeFYT0u",
      "authorship_tag": "ABX9TyNfELKfKQwvuIOsd2X92tPz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import numpy\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import re\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "except ImportError:\n",
        "    print('You are missing some packages! ' \\\n",
        "          'We will try installing them before continuing!')\n",
        "    !pip install \"numpy\" \"pandas\" \"sklearn\" \"nltk\"\n",
        "    import numpy\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import re\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZOgo5z54SPZ",
        "outputId": "a18542a2-d7c2-430f-ef70-09e5efbf8ef5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the stemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "-PMFLqGb5mBS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the patents data\n",
        "PATH_PATENTS = \"/content/drive/MyDrive/content_recommender_data/ABSTRACTS.csv\"\n",
        "patents = pd.read_csv(PATH_PATENTS)\n",
        "patents = patents[['ABSTRACTS']].dropna()  # select the 'abstract' column and drop rows with missing values\n",
        "abstracts = patents['ABSTRACTS'].tolist()  # convert the abstracts to a list"
      ],
      "metadata": {
        "id": "4lwQPm2K5o67"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk  # Don't forget to import nltk\n",
        "\n",
        "# Clean and tokenize the abstracts\n",
        "def clean_tokenize(document):\n",
        "    document = re.sub('[^\\w_\\s-]', ' ', document)  # remove punctuation marks and other symbols\n",
        "    tokens = nltk.word_tokenize(document)  # tokenize sentences\n",
        "    cleaned_abstract = ' '.join([stemmer.stem(item) for item in tokens])  # stem each token\n",
        "    return cleaned_abstract\n",
        "\n",
        "cleaned_abstracts = list(map(clean_tokenize, abstracts))"
      ],
      "metadata": {
        "id": "rwndcpG95tfB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input for keywords separated by commas\n",
        "user_keywords = input(\"Enter keywords separated by commas: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XygFxjq5wMy",
        "outputId": "2263b9c9-5428-4911-a73d-3d0cdd85a5ec"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter keywords separated by commas: teeth, dental, doctor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split user input into a list of keywords\n",
        "user_keywords_list = [keyword.strip() for keyword in user_keywords.split(',')]"
      ],
      "metadata": {
        "id": "XVpGcxMF5xjY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine keywords into a single string for processing\n",
        "cleaned_user_keywords = ' '.join(user_keywords_list)"
      ],
      "metadata": {
        "id": "V9KC6Upy5y8-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process and clean user input keywords\n",
        "cleaned_user_keywords = clean_tokenize(cleaned_user_keywords)"
      ],
      "metadata": {
        "id": "hon-SPYw501i"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate TF-IDF matrix for user input keywords and all patents\n",
        "tfidf_matrix = TfidfVectorizer(stop_words='english', min_df=2)\n",
        "abstract_tfidf_matrix = tfidf_matrix.fit_transform(cleaned_abstracts)\n",
        "user_keywords_tfidf_vector = tfidf_matrix.transform([cleaned_user_keywords])"
      ],
      "metadata": {
        "id": "Y5pXyX7v52Zi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between user input keywords and all patents\n",
        "patents_similarity_score = cosine_similarity(abstract_tfidf_matrix, user_keywords_tfidf_vector)\n",
        "recommended_patents_id = patents_similarity_score.flatten().argsort()[::-1]"
      ],
      "metadata": {
        "id": "KfQcGCuK539a"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the top N recommended patents with full abstract content\n",
        "recommended_patents = patents.loc[patents.index.isin(top_patents_indices), 'ABSTRACTS']\n",
        "\n",
        "print(f'\\nTop {num_recommendations} Recommended Patents:')\n",
        "for idx, abstract in zip(recommended_patents.index, recommended_patents):\n",
        "    print(f'\\nPatent ID: {idx}\\nAbstract: {abstract}\\n{\"-\"*50}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "UMA73oQN9own",
        "outputId": "c26bb958-fd77-4342-9dce-b1a29f8c7cc9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-27fb0742ec90>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display the top N recommended patents with full abstract content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrecommended_patents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_patents_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ABSTRACTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nTop {num_recommendations} Recommended Patents:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommended_patents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecommended_patents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'top_patents_indices' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import numpy\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import re\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "except ImportError:\n",
        "    print('You are missing some packages! ' \\\n",
        "          'We will try installing them before continuing!')\n",
        "    !pip install \"numpy\" \"pandas\" \"sklearn\" \"nltk\"\n",
        "    import numpy\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import re\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print('Done!')\n",
        "\n",
        "# Set up the stemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Load the patents data\n",
        "PATH_PATENTS = \"/content/drive/MyDrive/content_recommender_data/ABSTRACTS.csv\"\n",
        "patents = pd.read_csv(PATH_PATENTS)\n",
        "patents = patents[['ABSTRACTS']].dropna()  # select the 'abstract' column and drop rows with missing values\n",
        "abstracts = patents['ABSTRACTS'].tolist()  # convert the abstracts to a list\n",
        "\n",
        "# Clean and tokenize the abstracts\n",
        "def clean_tokenize(document):\n",
        "    document = re.sub('[^\\w_\\s-]', ' ', document)  # remove punctuation marks and other symbols\n",
        "    tokens = nltk.word_tokenize(document)  # tokenize sentences\n",
        "    cleaned_abstract = ' '.join([stemmer.stem(item) for item in tokens])  # stem each token\n",
        "    return cleaned_abstract\n",
        "\n",
        "cleaned_abstracts = list(map(clean_tokenize, abstracts))\n",
        "\n",
        "# Get user input for keywords separated by commas\n",
        "user_keywords = input(\"Enter keywords separated by commas: \")\n",
        "\n",
        "# Split user input into a list of keywords\n",
        "user_keywords_list = [keyword.strip() for keyword in user_keywords.split(',')]\n",
        "\n",
        "# Combine keywords into a single string for processing\n",
        "cleaned_user_keywords = ' '.join(user_keywords_list)\n",
        "\n",
        "# Process and clean user input keywords\n",
        "cleaned_user_keywords = clean_tokenize(cleaned_user_keywords)\n",
        "\n",
        "# Generate TF-IDF matrix for user input keywords and all patents\n",
        "tfidf_matrix = TfidfVectorizer(stop_words='english', min_df=2)\n",
        "abstract_tfidf_matrix = tfidf_matrix.fit_transform(cleaned_abstracts)\n",
        "user_keywords_tfidf_vector = tfidf_matrix.transform([cleaned_user_keywords])\n",
        "\n",
        "# Calculate cosine similarity between user input keywords and all patents\n",
        "patents_similarity_score = cosine_similarity(abstract_tfidf_matrix, user_keywords_tfidf_vector)\n",
        "recommended_patents_id = patents_similarity_score.flatten().argsort()[::-1]\n",
        "\n",
        "# Define the number of recommendations you want to display\n",
        "num_recommendations = 5\n",
        "\n",
        "# Display the top N recommended patents with full abstract content\n",
        "recommended_patents = patents.loc[patents.index.isin(recommended_patents_id[:num_recommendations]), 'ABSTRACTS']\n",
        "\n",
        "print(f'\\nTop {num_recommendations} Recommended Patents:')\n",
        "for idx, abstract in zip(recommended_patents.index, recommended_patents):\n",
        "    print(f'\\nPatent ID: {idx}\\nAbstract: {abstract}\\n{\"-\"*50}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLxEL4PI_vJE",
        "outputId": "42fab159-bbac-49ca-88f4-1a60fc7a8141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    }
  ]
}